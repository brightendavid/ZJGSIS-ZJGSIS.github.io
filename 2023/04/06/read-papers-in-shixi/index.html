<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>readpapers in shixi | brightendavid's blog</title><meta name="author" content="brightendavid"><meta name="copyright" content="brightendavid"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="[TOC] lora解决了如今语音识别等模型过大，需要频繁进行fine tune的问题  原本设计用于语音模型的微调 它冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到Transformer架构的每一层，大大减少了下游任务的可训练参数的数量 https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;microsof"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://brightendavid.github.io/2023/04/06/read-papers-in-shixi/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'readpapers in shixi',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-05 20:18:36'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img2.huashi6.com/images/resource/thumbnail/2022/06/21/104946_78617540821.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="brightendavid's blog"><span class="site-name">brightendavid's blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">readpapers in shixi</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-04-06T05:47:40.000Z" title="Created 2023-04-06 13:47:40">2023-04-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-06-05T12:18:36.886Z" title="Updated 2023-06-05 20:18:36">2023-06-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI%EF%BC%8C%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/">AI，信息安全</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="readpapers in shixi"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<h1><span id="lora">lora</span></h1><p>解决了如今语音识别等模型过大，需要频繁进行fine tune的问题</p>
<ul>
<li>原本设计用于语音模型的微调</li>
<li>它冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到<strong>Transformer架构的每一层</strong>，大大减少了下游任务的可训练参数的数量</li>
<li><a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/612992813">https://zhuanlan.zhihu.com/p/612992813</a>  stable diffusers各模型优化方法解读</li>
</ul>
<h2><span id="主要相关方法">主要相关方法</span></h2><p>看出了一些端倪</p>
<p>AIGC任务分为两个网络：text_embedding模块和Diffusion Model,会有一些模型将这两个模块分别加载。但是在webui中，为合并加载，应当使用只在stable diffusion模块中加入影响的Lora模型进行训练。</p>
<h3><span id="dreambooth">dreambooth</span></h3><ul>
<li>直接在原本大模型后插入SKS小网络优化，微调参数。</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-991e6410b7c386b8b14864e12c982335_720w.webp" alt="img"></p>
<h3><span id="lora">lora</span></h3><ul>
<li><p>注意，只是在text_embedding后加入一个小模块</p>
</li>
<li><p>在基础模型sd的transformer层之间插入秩分解矩阵，固定原本基础模型的参数。</p>
</li>
<li><p>好处在于可以得到一个较小的lora微调模型。无论是传播还是使用都方便</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://civitai.com/">https://civitai.com/</a>  lora模型站</p>
</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-8e7e27d7a8b5514e756395bf0e8efde9_720w.webp" alt="img"></p>
<h3><span id="hypernetwork">Hypernetwork</span></h3><ul>
<li>基本原理和lora相同，输出模型为全部的模型，即transformer块间加入微调层的大模型</li>
</ul>
<h2><span id="lora">lora</span></h2><p>提出LoRA，一种有效的自适应策略，既没有引入推理延迟，也没有减少输入序列长度，同时保持高模型质量。重要的是，当作为服务部署时，通过共享绝大多数模型参数，它允许快速任务切换。虽然我们关注的是Transformer语言模型，但所提出的原则通常适用于任何具有密集层的神经网络。</p>
<h1><span id="high-resolution-image-synthesis-with-latent-diffusion-models"><strong>High-Resolution Image Synthesis with Latent Diffusion Models</strong></span></h1><ul>
<li><p>作者：Ludwig Maximilian University of Munich &amp; IWR，慕尼黑大学</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p>
</li>
</ul>
<p>我们的潜在扩散模型(ldm)在图像修补和类条件图像合成方面取得了新的最先进的分数，并在各种任务上获得了极具竞争力的性能，包括文本到图像合成，无条件图像生成和超分辨率，同时与基于像素的DMs相比，大大降低了计算需求。</p>
<h1><span id="pcat-functionality-and-data-stealing-from-split-learning-by-pseudo-client-attack"><strong>PCAT: Functionality and Data Stealing from Split Learning by Pseudo-Client Attack</strong></span></h1><ul>
<li><p>一种split learning攻击方法</p>
</li>
<li><p>现有方法需要白盒攻击，并只对浅层网络有效</p>
</li>
<li><p>思路在于生成一个类客户机模型，窃取客户机的功能</p>
</li>
</ul>
<h2><span id="创新">创新</span></h2><ul>
<li>可用于split learn 的各种结构</li>
<li>不需要得到网络结构</li>
<li>攻击对于服务器是透明的，行为与正常相同</li>
<li>只需要少量的原始数据</li>
</ul>
<h2><span id="split-learning">split learning</span></h2><p><img src="/2023/04/06/magicer-under-paper/image-20230322151413218.png" alt="image-20230322151413218"></p>
<ul>
<li>split learning：多个常规算力节点(Alices)+一个超级算力节点(Bob)。核心思想是各方在不泄露原始数据的情况下，共同训练一个完整的模型，同时将模型中计算负载较高的部分安排在Bob节点。对Split Learning的介绍一般是基于分布式计算架构SplitNN进行展开。</li>
<li>联邦学习：分布式计算，多方提供部分数据集，地位相同</li>
</ul>
<p>看不懂这个</p>
<h1><span id="联邦学习-federated-learning-algorithm">联邦学习 （federated learning algorithm）</span></h1><ul>
<li>服务器是诚实但好奇的。</li>
<li>攻击形式：成员推理攻击、模型反演&#x2F;数据重构攻击和属性推理攻击。可以攻击获得训练图像和模型权重</li>
<li>可以使用梯度模糊，加入高斯噪声等方法防御攻击，梯度压缩[61]，差分私有训练[62]，和表示扰动[66]。</li>
</ul>
<h1><span id="knowledge-distillation知识蒸馏">knowledge distillation（知识蒸馏）</span></h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102038521">https://zhuanlan.zhihu.com/p/102038521</a></p>
<ul>
<li><p>模型压缩方法，是一种基于“教师-学生网络思想”的训练方法。</p>
</li>
<li><p>KD:就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Distill”)提取到另一个模型里面去。</p>
</li>
<li><p>可以用于模型的轻量化，类似的还有模型剪枝</p>
</li>
</ul>
<blockquote>
<p>模型就像一个容器，训练数据中蕴含的知识就像是要装进容器里的水。当数据知识量(水量)超过模型所能建模的范围时(容器的容积)，加再多的数据也不能提升效果(水再多也装不进容器)，因为模型的表达空间有限(容器容积有限)，就会造成<strong>underfitting</strong>；而当模型的参数量大于已有知识所需要的表达空间时(容积大于水量，水装不满容器)，就会造成<strong>overfitting</strong>，即模型的variance会增大(想象一下摇晃半满的容器，里面水的形状是不稳定的)。</p>
</blockquote>
<p>但是可以通过合理的训练方法获取更多的知识。</p>
<blockquote>
<p>知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p>
<ol>
<li>原始模型训练: 训练”Teacher模型”,  简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li>
<li>精简模型训练: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li>
</ol>
</blockquote>
<h2><span id="关键点">关键点</span></h2><ul>
<li><strong>机器学习最根本的目的</strong>在于训练出在某个问题上泛化能力强的模型。</li>
</ul>
<ul>
<li><strong>泛化能力强</strong>: 在某问题的所有数据上都能很好地反应输入和输出之间的关系，无论是训练数据，还是测试数据，还是任何属于该问题的未知数据。</li>
<li>负标签也包含了信息，和网络的泛化性能有关</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-d01f5142d06aa27bc5e207831b5131d9_720w.webp" alt="img"></p>
<p><img src="https://pic4.zhimg.com/80/v2-a9e90626c5ac6f64a7e04c89f6ce3013_720w.webp" alt="img"></p>
<ul>
<li><p><strong>温度的高低改变的是Net-S训练过程中对负标签的关注程度</strong>: 温度较低时，对负标签的关注，尤其是那些显著低于平均值的负标签的关注较少；而温度较高时，负标签相关的值会相对增大，Net-S会相对多地关注到负标签。</p>
</li>
<li><p>矛盾点在于</p>
<blockquote>
<p>从有部分信息量的负标签中学习 –&gt; 温度要高一些</p>
<p>防止受负标签中噪声的影响 –&gt;温度要低一些</p>
</blockquote>
</li>
</ul>
<h2><span id="训练">训练</span></h2><p><strong>第一步</strong>是训练Net-T；<strong>第二步</strong>是在高温T下，蒸馏Net-T的知识到Net-S；推理过程使用T&#x3D;1.</p>
<h1><span id="剪枝">剪枝</span></h1><ul>
<li>删除不重要的权重，比如y&#x3D;x+2*x^2 ，可以去除x的权重</li>
<li>深度学习模型中一般存在大量的冗余参数，模型剪枝即删除不重要的权重，可以缩减模型大小，提高模型计算效率。剪枝算法可以分为以下三个步骤：训练模型、模型剪枝、重新训练，并迭代以上三个步骤，直到模型精度达到目标。根据剪枝颗粒的不同，模型剪枝方法可以分为细粒度剪枝、向量剪枝、卷积核剪枝、滤波器剪枝。</li>
</ul>
<h1><span id="神经网络水印">神经网络水印</span></h1><p>证明网络权重的产权问题</p>
<h2><span id="分类">分类</span></h2><ul>
<li>基于内在机制的白盒神经网络水印</li>
<li>基于触发集的黑盒神经网络水印</li>
<li>基于输出结果的无盒神经网络水印</li>
</ul>
<h2><span id="白盒待修">白盒（待修</span></h2><p>修改网络权重，加入一个符合正态分布的全连接层，在这个全连接层中加入秘密信息（不明）</p>
<h2><span id="黑盒待修">黑盒（待修</span></h2><p>基于触发集，对于特定一系列图像有特定的输出，有点像对抗样本</p>
<ul>
<li>问题在于只能在分类任务中使用</li>
</ul>
<h2><span id="无盒">无盒</span></h2><blockquote>
<p>Watermarking Neural Networks with Watermarked Images</p>
<p> DOI 10.1109&#x2F;TCSVT.2020.3030671</p>
<p>复日大学</p>
<p>Hanzhou Wu, <em>Member, IEEE</em>, Gen Liu, Yuwei Yao and Xinpeng Zhang</p>
</blockquote>
<p>基于输出图像，效果和“完成输出之后，将图像输入一个带密钥的水印网络相似”</p>
<p>区别在于</p>
<table>
<thead>
<tr>
<th>无盒水印</th>
<th>后加水印</th>
</tr>
</thead>
<tbody><tr>
<td>遗失的权重还具有水印</td>
<td>遗失无效</td>
</tr>
</tbody></table>
<p><img src="/2023/04/06/read-papers-in-shixi/image-20230327142820987.png" alt="image-20230327142820987"></p>
<p>感觉这个鲁棒性的问题很大，可能噪声鲁棒性是网络训练完就自带的，但是剪切的鲁棒性没有加入noise layer不大可能有。</p>
<h1><span id="模型所有权技术">模型所有权技术</span></h1><ul>
<li><p>Fingerprint：hash&#x2F;md5</p>
</li>
<li><p>模型相似度:Copy, Right? A Testing Framework for Copyright</p>
<p>Protection of Deep Learning Models</p>
</li>
</ul>
<h1><span id="可逆神经网络inn-watermark">可逆神经网络INN-watermark</span></h1><p>Invertible Neural Networks, INN</p>
<p>图像质量指标：PSNR(dB)，SSIM ,MAE，RMSE</p>
<blockquote>
<p><strong>HiNet: Deep Image Hiding by Invertible Network</strong></p>
<p>beihang unversity</p>
<p> github:<a target="_blank" rel="noopener" href="https://github.com/TomTomTommi/HiNet">https://github.com/TomTomTommi/HiNet</a></p>
<p>数据集：<em>ImageNet, COCO and DIV2K datasets.</em></p>
</blockquote>
<p>可逆神经网络，编解码部分使用相同的参数，可以减少模型大小</p>
<p>水印问题，编解码可以视为可逆问题。</p>
<p><img src="/2023/04/06/read-papers-in-shixi/image-20230327143907338.png" alt="image-20230327143907338"></p>
<p><img src="/2023/04/06/read-papers-in-shixi/image-20230327144123731.png" alt="image-20230327144123731"></p>
<p><img src="https://img-blog.csdnimg.cn/adaeca37cdf241debc14bb3d21fec57d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAUGhvZW5peHRyZWVfRG9uZ1poYW8=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img"></p>
<h2><span id="dwtiwt">DWT,IWT</span></h2><p>离散小波变换，具有可逆性，逆小波变换。制造原始图像的高频分量，隐写域在图像的高频分量中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DWT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DWT, self).__init__()</span><br><span class="line">        self.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> dwt_init(x)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dwt_init</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># dwt 离散小波变换，获取4个分量</span></span><br><span class="line">    x01 = x[:, :, <span class="number">0</span>::<span class="number">2</span>, :] / <span class="number">2</span></span><br><span class="line">    x02 = x[:, :, <span class="number">1</span>::<span class="number">2</span>, :] / <span class="number">2</span></span><br><span class="line">    x1 = x01[:, :, :, <span class="number">0</span>::<span class="number">2</span>]</span><br><span class="line">    x2 = x02[:, :, :, <span class="number">0</span>::<span class="number">2</span>]</span><br><span class="line">    x3 = x01[:, :, :, <span class="number">1</span>::<span class="number">2</span>]</span><br><span class="line">    x4 = x02[:, :, :, <span class="number">1</span>::<span class="number">2</span>]</span><br><span class="line">    x_LL = x1 + x2 + x3 + x4</span><br><span class="line">    x_HL = -x1 - x2 + x3 + x4</span><br><span class="line">    x_LH = -x1 + x2 - x3 + x4</span><br><span class="line">    x_HH = x1 - x2 - x3 + x4</span><br><span class="line">    <span class="keyword">return</span> torch.cat((x_LL, x_HL, x_LH, x_HH), <span class="number">1</span>) <span class="comment"># 堆叠4个分量 (:,12,:,:)</span></span><br></pre></td></tr></table></figure>

<h2><span id="inn代码实现">INN代码实现</span></h2><h3><span id="主网络">主网络</span></h3><p>主要特点为编解码可逆</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, rev=<span class="literal">False</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> rev:</span><br><span class="line">        out = self.inv1(x)</span><br><span class="line">        out = self.inv2(out)</span><br><span class="line">        out = self.inv3(out)</span><br><span class="line">        out = self.inv4(out)</span><br><span class="line">        out = self.inv5(out)</span><br><span class="line">        out = self.inv6(out)</span><br><span class="line">        out = self.inv7(out)</span><br><span class="line">        out = self.inv8(out)</span><br><span class="line"></span><br><span class="line">        out = self.inv9(out)</span><br><span class="line">        out = self.inv10(out)</span><br><span class="line">        out = self.inv11(out)</span><br><span class="line">        out = self.inv12(out)</span><br><span class="line">        out = self.inv13(out)</span><br><span class="line">        out = self.inv14(out)</span><br><span class="line">        out = self.inv15(out)</span><br><span class="line">        out = self.inv16(out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        out = self.inv16(x, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv15(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv14(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv13(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv12(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv11(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv10(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv9(out, rev=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        out = self.inv8(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv7(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv6(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv5(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv4(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv3(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv2(out, rev=<span class="literal">True</span>)</span><br><span class="line">        out = self.inv1(out, rev=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<h3><span id="inv_block">INV_block</span></h3><p>φ ρ η网络是相同的残差块，参数不共享</p>
<p>在解码网络中，直接设置rev&#x3D;true</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">INV_block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, subnet_constructor=ResidualDenseBlock_out, clamp=c.clamp, harr=<span class="literal">True</span>, in_1=<span class="number">3</span>, in_2=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> harr:</span><br><span class="line">            self.split_len1 = in_1 * <span class="number">4</span></span><br><span class="line">            self.split_len2 = in_2 * <span class="number">4</span></span><br><span class="line">        self.clamp = clamp</span><br><span class="line">        <span class="comment"># ρ</span></span><br><span class="line">        self.r = subnet_constructor(self.split_len1, self.split_len2)</span><br><span class="line">        <span class="comment"># η</span></span><br><span class="line">        self.y = subnet_constructor(self.split_len1, self.split_len2)</span><br><span class="line">        <span class="comment"># φ</span></span><br><span class="line">        self.f = subnet_constructor(self.split_len2, self.split_len1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">e</span>(<span class="params">self, s</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.exp(self.clamp * <span class="number">2</span> * (torch.sigmoid(s) - <span class="number">0.5</span>))  <span class="comment">#e^ (clamp* 2 *  1/(1+e^(-x)) -0.5)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, rev=<span class="literal">False</span></span>):</span><br><span class="line">        x1, x2 = (x.narrow(<span class="number">1</span>, <span class="number">0</span>, self.split_len1),</span><br><span class="line">                  x.narrow(<span class="number">1</span>, self.split_len1, self.split_len2))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> rev: <span class="comment"># rev 反转，解码网络部分</span></span><br><span class="line"></span><br><span class="line">            t2 = self.f(x2)</span><br><span class="line">            y1 = x1 + t2</span><br><span class="line">            s1, t1 = self.r(y1), self.y(y1)</span><br><span class="line">            y2 = self.e(s1) * x2 + t1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">            s1, t1 = self.r(x1), self.y(x1)</span><br><span class="line">            y2 = (x2 - t1) / self.e(s1)</span><br><span class="line">            t2 = self.f(y2)</span><br><span class="line">            y1 = (x1 - t2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.cat((y1, y2), <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2023/04/06/read-papers-in-shixi/image-20230327152145155.png" alt="image-20230327152145155"></p>
<h1><span id="error-subprocess-exited-with-error错误解决方案即pip-安装第三方包问题">error: subprocess-exited-with-error错误解决方案，即pip 安装第三方包问题</span></h1><p>有些说是pip版本不对</p>
<p>实际上，安装不上包的原因有很多，这次是因为内存不足，无法安装造成的报错，删除一些不用的包或者不用的大文件即可</p>
<h1><span id="安装torch脚本重要">安装torch脚本（重要）</span></h1><ul>
<li>注意不要漏打</li>
</ul>
<ul>
<li>python 版本 应该高点，3.10左右吧，pip install torch&#x3D;&#x3D;1.12.1+cu116 torchvision&#x3D;&#x3D;0.13.1+cu116 –extra-index-url <a target="_blank" rel="noopener" href="https://download.pytorch.org/whl/cu116">https://download.pytorch.org/whl/cu116</a></li>
</ul>
<ul>
<li>python&#x3D;&#x3D;3.8,pip install torch&#x3D;&#x3D;1.8.1+cu101 torchvision&#x3D;&#x3D;0.9.1+cu101  –extra-index-url <a target="_blank" rel="noopener" href="https://download.pytorch.org/whl/cu101">https://download.pytorch.org/whl/cu101</a></li>
<li><a target="_blank" rel="noopener" href="https://download.pytorch.org/whl/cu117%E8%AF%A6%E8%A7%81">https://download.pytorch.org/whl/cu117详见</a></li>
</ul>
<h1><span id="生成requirementstxt">生成requirements.txt</span></h1><blockquote>
<p>pip install pipreqs</p>
<p>pipreqs .</p>
<p> 或限定编码方式为utf8，否则会有编码错误error</p>
<p>pipreqs .&#x2F; –encoding&#x3D;utf8</p>
</blockquote>
<h1><span id="a-survey-of-large-language-models">A Survey of Large Language Models</span></h1><p>三步：pre-training ,adaptation tuning,design suitable prompting strategies</p>
<p>即：预训练，调优，和提示策略</p>
<h2><span id="调优">调优</span></h2><p>RLHF系统主要由三个关键部分组成：预先训练好的LM，从人类反馈中学习的奖励模型，以及训练LM的RL算法（RL强化学习）</p>
<h3><span id="提示策略">提示策略</span></h3><ul>
<li><strong>in-context learning</strong></li>
</ul>
<p>ICL：上下文学习in-context learning</p>
<p>ICL用一个自然语言描述、几个演示程序和一个测试查询来提示llm。</p>
<h2><span id="cot"><strong>CoT</strong></span></h2><ul>
<li><strong>Chain-of-Thought Prompting</strong>（CoT）</li>
</ul>
<p>CoT提示则涉及到提示中的一系列中间推理步骤。</p>
<blockquote>
<p>推理步骤：</p>
<p>If a rectangle has a length of 6 cm and a width of 3 cm, </p>
<p>what is the perimeter of the rectangle?</p>
<p>For a rectangle, add up the length and width and double it. </p>
<p>So, the perimeter of this rectangle is (6 + 3) x 2 &#x3D; 18 cm.</p>
</blockquote>
<h2><span id="改进cot">改进CoT</span></h2><ul>
<li><strong>Few-shot CoT.</strong></li>
</ul>
<p><em><strong>Enhanced CoT strategies</strong></em></p>
<p><strong>自一致性策略</strong></p>
<p>作为改进的CoT策略，生成多个推理路径，所有推理结果获得一个集合，投票决定</p>
<ul>
<li><strong>Zero-shot CoT.</strong></li>
</ul>
<p>零镜头CoT在提示中不包含人工注释的任务演示。相反，它直接生成推理步骤，然后使用生成的cot来推导答案。</p>
<p><strong>模型专门化</strong></p>
<blockquote>
<p> In addition to directly utilizing LLMs with ICL and CoT, some recent studies explore how to specialize the ability of LLMs towards specifific tasks [255–257], which is called <em>model specialization</em> [258]. For example, the researchers in [258] specialize the ability of mathematical reasoning from LLMs through fifine-tuning the small-scale Flan-T5 [81] on CoT reasoning paths generated by LLMs. Model specialization can also be applied to solve a variety of tasks like question answering [259], code synthesis [260], and information retrieval [261].</p>
</blockquote>
<p>模型专门化是本次任务的重点，相对于一种通用的LLM模型,应当获取一个应用于特定领域的语音模型</p>
<h2><span id="现有语音模型任务">现有语音模型任务</span></h2><p>现有的语言生成任务大致可以分为语言建模任务、条件文本生成任务和代码合成任务。</p>
<h2><span id="llm使用知识-knowledge-utilization">LLM使用知识  <em>Knowledge Utilization</em></span></h2><h3><span id="closed-book-qa"><strong>Closed-Book QA</strong></span></h3><p>闭卷问答</p>
<p>只能使用train前用于训练的基础知识，如自然问题[283]、Web问题[286]和TriviaQA [287]</p>
<p>只能根据上下文回答</p>
<h3><span id="open-book-qa"><strong>open-Book QA</strong></span></h3><p>开卷问答</p>
<p>回答过程是可以联网的</p>
<p>open-book QA数据集（例如，自然问题[283]、OpenBookQA [295]和SQuAD [298]）与封闭书QA数据集有重叠，但它们包含了外部数据源，例如，维基百科</p>
<h3><span id="knowledge-completion"><strong>Knowledge Completion.</strong></span></h3><h2><span id="问题-hallucination-幻觉产生">问题： <em>Hallucination</em>  （幻觉产生）</span></h2><ul>
<li><p>就是LLM模型骗人，胡乱生成回答内容。在所有的LLM模型中都存在</p>
</li>
<li><p>本质上讲，llm似乎是“无意识地”利用知识来解决任务，而这仍然缺乏准确控制内在或外部知识使用的能力</p>
</li>
<li><p>评估幻觉问题，人们提出了一组幻觉检测任务，如真实QA[285]，用于检测被模型模拟的人类谎言</p>
</li>
</ul>
<h2><span id="问题knowledge-recency知识更新">问题：<em>Knowledge recency</em>（知识更新）</span></h2><p>新的知识和重新训练过程中的灾难性遗忘问题</p>
<p>通过外接搜索引擎可以缓解这个问题</p>
<h2><span id="复杂推理">复杂推理</span></h2><p>复杂推理是指理解和利用支持证据或逻辑得出结论或做出决策的能力。根据推理过程中所涉及的逻辑和证据的类型，我们考虑将现有的评价任务分为知识推理、符号推理和数学推理三类</p>
<h3><span id="知识推理">知识推理</span></h3><p>LLMs may have diffificulty in explicitly inferring the commonsense knowledge required by a specifific task, though they can successfully solve it.</p>
<h3><span id="符号推理">符号推理</span></h3><h3><span id="数学推理">数学推理</span></h3><h1><span id="vits">VITS</span></h1></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="http://brightendavid.github.io">brightendavid</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://brightendavid.github.io/2023/04/06/read-papers-in-shixi/">http://brightendavid.github.io/2023/04/06/read-papers-in-shixi/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%AC%E7%A7%91%E7%94%9F/">本科生</a></div><div class="post_share"><div class="social-share" data-image="https://img2.huashi6.com/images/resource/thumbnail/2022/06/21/104946_78617540821.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/04/18/ssh-and-ssl/" title="ssh and ssl"><img class="cover" src="http://img9.vilipix.com/picture/pages/original/2021/07/18/09/01/82100144_p0.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">ssh and ssl</div></div></a></div><div class="next-post pull-right"><a href="/2023/04/02/C/" title="c 语言"><img class="cover" src="https://img2.huashi6.com/images/resource/thumbnail/2022/04/01/173230_55077196870.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">c 语言</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/04/02/C/" title="c 语言"><img class="cover" src="https://img2.huashi6.com/images/resource/thumbnail/2022/04/01/173230_55077196870.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-02</div><div class="title">c 语言</div></div></a></div><div><a href="/2022/08/08/film-to-picture/" title="film to picture"><img class="cover" src="https://img2.huashi6.com/images/resource/thumbnail/2022/06/21/104946_78617540821.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-08</div><div class="title">film to picture</div></div></a></div><div><a href="/2022/08/24/how-to-win-the-champion/" title="how to win the champion"><img class="cover" src="http://img9.vilipix.com/picture/pages/original/2021/07/18/09/01/82100144_p0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-24</div><div class="title">how to win the champion</div></div></a></div><div><a href="/2020/05/27/%E5%8E%9F%E5%88%9B--%20%202021-3-21%20%E6%95%B0%E6%8D%AE%E5%8C%85%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%B4%A2BUUCTF/" title="2021-3-21 数据包中的线索BUUCTF"><img class="cover" src="https://img2.huashi6.com/images/resource/2019/04/23/h74336040p0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-05-27</div><div class="title">2021-3-21 数据包中的线索BUUCTF</div></div></a></div><div><a href="/2020/05/27/%E5%8E%9F%E5%88%9B--%20%2032%E4%BD%8D%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80%E5%A4%8D%E4%B9%A0%E6%96%87%E6%A1%A3/" title="32位汇编语言复习文档"><img class="cover" src="https://img2.huashi6.com/images/resource/thumbnail/2022/04/01/173230_55077196870.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-05-27</div><div class="title">32位汇编语言复习文档</div></div></a></div><div><a href="/2020/05/27/%E5%8E%9F%E5%88%9B--%20%20BUUCTF_FLAG_MISC/" title="BUUCTF_FLAG_MISC"><img class="cover" src="https://img2.huashi6.com/images/resource/2020/05/07/81376h525p0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-05-27</div><div class="title">BUUCTF_FLAG_MISC</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-map"><div class="card-content"><div class="item-headline"><i class="fas fa-globe-asia" aria-hidden="true"></i><span>访客地图</span></div><script id="clstr_globe" type="text/javascript" defer="defer" src="//clustrmaps.com/globe.js?d=2oJ_tAjZh9xa28T3L7EXz0QNXKwCMd6ruM6IbGewPCA"></script><script id="clustrmaps" type="text/javascript" defer="defer" src="******************"></script></div></div><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">brightendavid</div><div class="author-info__description">马达马达得思,but Shambhala is not far away.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/brightendavid"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Ciallo～(∠・ω< )⌒★. I am a college student majoring in IS and CS.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">lora</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.1.</span> <span class="toc-text">主要相关方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.1.1.</span> <span class="toc-text">dreambooth</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.1.2.</span> <span class="toc-text">lora</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.1.3.</span> <span class="toc-text">Hypernetwork</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.2.</span> <span class="toc-text">lora</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">High-Resolution Image Synthesis with Latent Diffusion Models</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">PCAT: Functionality and Data Stealing from Split Learning by Pseudo-Client Attack</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">3.1.</span> <span class="toc-text">创新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">3.2.</span> <span class="toc-text">split learning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">联邦学习 （federated learning algorithm）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">knowledge distillation（知识蒸馏）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">5.1.</span> <span class="toc-text">关键点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">5.2.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">剪枝</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">神经网络水印</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">7.1.</span> <span class="toc-text">分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">7.2.</span> <span class="toc-text">白盒（待修</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">7.3.</span> <span class="toc-text">黑盒（待修</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">7.4.</span> <span class="toc-text">无盒</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">模型所有权技术</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">9.</span> <span class="toc-text">可逆神经网络INN-watermark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">9.1.</span> <span class="toc-text">DWT,IWT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">9.2.</span> <span class="toc-text">INN代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">9.2.1.</span> <span class="toc-text">主网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">9.2.2.</span> <span class="toc-text">INV_block</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">10.</span> <span class="toc-text">error: subprocess-exited-with-error错误解决方案，即pip 安装第三方包问题</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">11.</span> <span class="toc-text">安装torch脚本（重要）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">12.</span> <span class="toc-text">生成requirements.txt</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">13.</span> <span class="toc-text">A Survey of Large Language Models</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">13.1.</span> <span class="toc-text">调优</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">13.1.1.</span> <span class="toc-text">提示策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">13.2.</span> <span class="toc-text">CoT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">13.3.</span> <span class="toc-text">改进CoT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">13.4.</span> <span class="toc-text">现有语音模型任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">13.5.</span> <span class="toc-text">LLM使用知识  Knowledge Utilization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">13.5.1.</span> <span class="toc-text">Closed-Book QA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">13.5.2.</span> <span class="toc-text">open-Book QA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">13.5.3.</span> <span class="toc-text">Knowledge Completion.</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">13.6.</span> <span class="toc-text">问题： Hallucination  （幻觉产生）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">13.7.</span> <span class="toc-text">问题：Knowledge recency（知识更新）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">13.8.</span> <span class="toc-text">复杂推理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">13.8.1.</span> <span class="toc-text">知识推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">13.8.2.</span> <span class="toc-text">符号推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">13.8.3.</span> <span class="toc-text">数学推理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">14.</span> <span class="toc-text">VITS</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/06/12/%E8%80%83%E7%A0%94%E5%BF%83%E5%BE%97/" title="考研心得"><img src="http://img9.vilipix.com/picture/pages/original/2021/07/18/09/01/82100144_p0.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="考研心得"/></a><div class="content"><a class="title" href="/2023/06/12/%E8%80%83%E7%A0%94%E5%BF%83%E5%BE%97/" title="考研心得">考研心得</a><time datetime="2023-06-12T14:49:49.000Z" title="Created 2023-06-12 22:49:49">2023-06-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/12/StegaStamp%E7%BF%BB%E8%AF%91/" title="StegaStamp翻译"><img src="https://img2.huashi6.com/images/resource/2020/05/07/81376h525p0.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="StegaStamp翻译"/></a><div class="content"><a class="title" href="/2023/06/12/StegaStamp%E7%BF%BB%E8%AF%91/" title="StegaStamp翻译">StegaStamp翻译</a><time datetime="2023-06-12T14:49:33.000Z" title="Created 2023-06-12 22:49:33">2023-06-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/08/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/" title="学习计划"><img src="https://img2.huashi6.com/images/resource/2020/05/07/81376h525p0.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="学习计划"/></a><div class="content"><a class="title" href="/2023/06/08/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/" title="学习计划">学习计划</a><time datetime="2023-06-08T12:15:55.000Z" title="Created 2023-06-08 20:15:55">2023-06-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/06/qt/" title="qt"><img src="http://img9.vilipix.com/user/addition/1000275203/1645577549245_82255998_p0.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="qt"/></a><div class="content"><a class="title" href="/2023/06/06/qt/" title="qt">qt</a><time datetime="2023-06-06T07:23:00.000Z" title="Created 2023-06-06 15:23:00">2023-06-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E7%BA%AA%E8%A6%81/" title="论文写作纪要"><img src="https://img2.huashi6.com/images/resource/2019/04/23/h74336040p0.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文写作纪要"/></a><div class="content"><a class="title" href="/2023/05/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E7%BA%AA%E8%A6%81/" title="论文写作纪要">论文写作纪要</a><time datetime="2023-05-29T15:26:56.000Z" title="Created 2023-05-29 23:26:56">2023-05-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By brightendavid</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>